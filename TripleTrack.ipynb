{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 46\n",
      "Pre-processing...\n",
      "Sure um start off - I'm a Purdue grad\n",
      "off 3, a 7, \n",
      "\n",
      "i_stop:0\n",
      "i_sen:5\n",
      "stopNum:2\n",
      "i_stop:0\n",
      "i_sen:7\n",
      "i_stop:1\n",
      "i_sen:8\n",
      "a\n",
      "i_stop:1\n",
      "i_sen:9\n",
      "\n",
      "I - be - Purdue grad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# pre-processing\n",
    "def filter(senSet):\n",
    "    #remove content between [ ]\n",
    "    print(\"Pre-processing...\")\n",
    "    for index in range(len(senSet)):\n",
    "        while senSet[index].find('[')>=0:\n",
    "            i_start = senSet[index].find('[')\n",
    "            i_end = senSet[index].find(']')\n",
    "            s = senSet[index][i_start:i_end+2]\n",
    "            senSet[index] = senSet[index].replace(s, \"\")\n",
    "\n",
    "#load data\n",
    "file = open(\"shortdataset.csv\", \"r\")\n",
    "#file = open(\"newdataset_formatted.csv\", \"r\")\n",
    "reader = csv.reader(file)\n",
    "\n",
    "senSet = []\n",
    "for item in reader:\n",
    "    #format sentences in item as string\n",
    "    fullP = \"\".join(item)\n",
    "    splitP = fullP.split(\";\", 3);\n",
    "    splitS = splitP[3][1:len(splitP[3])].split(\".\");\n",
    "    #print(splitS)\n",
    "    for sen in splitS:\n",
    "        senSet.append(sen)#store the sentence into an array\n",
    "\n",
    "file.close()\n",
    "print(\"Total sentences: \" + str(len(senSet)))\n",
    "\n",
    "#pre-processing\n",
    "filter(senSet)\n",
    "\n",
    "'''for sentence in senSet:\n",
    "    print(senSet)'''\n",
    "\n",
    "#parse sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# for index in range(len(senSet)):\n",
    "index = 0\n",
    "doc = nlp(str(senSet[index]))\n",
    "print(senSet[index])\n",
    "\n",
    "#print the result\n",
    "sub = \"\"\n",
    "pred = \"\"\n",
    "obj = \"\"\n",
    "stop_index = []\n",
    "\n",
    "for token in doc:\n",
    "    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #      token.shape_, token.is_alpha, token.is_stop)\n",
    "    # record the index of stop words\n",
    "    if token.is_stop:\n",
    "        print(token.text + ' ' + str(token.i) + ',', end = ' ')\n",
    "        stop_index.append(token.i)\n",
    "    if re.match('nsubj', token.dep_):   \n",
    "        subj = token.text\n",
    "    if re.match('ROOT', token.dep_): \n",
    "        pred = token.lemma_\n",
    "        pred_orig = token.text\n",
    "    if re.match('dobj', token.dep_): \n",
    "        obj = token.lemma_\n",
    "print('\\n')\n",
    "        \n",
    "# using chunk to update subject and object\n",
    "for chunk in doc.noun_chunks:\n",
    "    if chunk.root.head.text == pred_orig and re.match('nsubj', chunk.root.dep_):\n",
    "        subj = chunk.text\n",
    "        # remove stop words\n",
    "        i_stop=0\n",
    "        for i_sen in range(chunk.start, chunk.end):\n",
    "            print('i_stop:' + str(i_stop))\n",
    "            print('i_sen:' + str(i_sen))\n",
    "            while i_stop < len(stop_index) and stop_index[i_stop] < i_sen-1:\n",
    "                #print(str(stop_index[i_stop]) + ' ' + str(i_sen))\n",
    "                i_stop = i_stop+1\n",
    "            # there is no stop word in current chunk\n",
    "            if i_stop >= len(stop_index):\n",
    "                break;\n",
    "            #print(i_sen)\n",
    "            # finish going through the chunk\n",
    "            if stop_index[i_stop] > chunk.end-1:\n",
    "                break\n",
    "            # find the stop word and remove it\n",
    "            if stop_index[i_stop] == i_sen-1:\n",
    "                print(doc[i_sen-1])\n",
    "                if i_sen-1 == chunk.start:\n",
    "                    subj = subj.replace(doc[i_sen-1].text + ' ', '')\n",
    "                else:\n",
    "                    subj = subj.replace(' ' + doc[i_sen-1].text, '')\n",
    "            \n",
    "    if chunk.root.head.text == pred_orig and re.match('dobj|attr', chunk.root.dep_):\n",
    "        obj = chunk.text\n",
    "        # remove stop words\n",
    "        i_stop=0\n",
    "        print('stopNum:' + str(len(stop_index)))\n",
    "        for i_sen in range(chunk.start, chunk.end):\n",
    "            print('i_stop:' + str(i_stop))\n",
    "            print('i_sen:' + str(i_sen))\n",
    "            while i_stop < len(stop_index) and stop_index[i_stop] < i_sen-1:\n",
    "                #print(str(stop_index[i_stop]) + ' ' + str(i_sen))\n",
    "                i_stop = i_stop+1\n",
    "            # there is no stop word in current chunk\n",
    "            if i_stop >= len(stop_index):\n",
    "                break;\n",
    "            #print(i_sen)\n",
    "            # finish going through the chunk\n",
    "            if stop_index[i_stop] > chunk.end-1:\n",
    "                break\n",
    "            # find the stop word and remove it\n",
    "            if stop_index[i_stop] == i_sen-1:\n",
    "                print(doc[i_sen-1])\n",
    "                if i_sen-1 == chunk.start:\n",
    "                    obj = obj.replace(doc[i_sen-1].text + ' ', '')\n",
    "                else:\n",
    "                    obj = obj.replace(' ' + doc[i_sen-1].text, '')\n",
    "            \n",
    "    #print(chunk.text + ' ' + str(chunk.start))\n",
    "    #print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "\n",
    "print('\\n' + subj + ' - ' + pred + ' - ' + obj + '\\n')\n",
    "\n",
    "## visualize the semantic tree\n",
    "#options = {'compact': True, 'color': 'blue'}\n",
    "#displacy.serve(doc, style='dep', options=options)\n",
    "#displacy.serve(doc, style='dep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
