{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary created!\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary based on all the words in the ontology\n",
    "\n",
    "import csv\n",
    "\n",
    "# load from original dataset\n",
    "with open('CSO.3.1_short.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    entitySet = set()\n",
    "    for item in reader:\n",
    "        # subject\n",
    "        if \"https://cso.kmi.open.ac.uk/topics/\" in item[0]:\n",
    "            entity = item[0].replace('https://cso.kmi.open.ac.uk/topics/', '')\n",
    "            entitySet.add(entity[1:-1])\n",
    "        \n",
    "        # object\n",
    "        if \"https://cso.kmi.open.ac.uk/topics/\" in item[2]:\n",
    "            entity = item[2].replace('https://cso.kmi.open.ac.uk/topics/', '')\n",
    "            entitySet.add(entity[1:-1])\n",
    "    #print (entitySet)\n",
    "\n",
    "# store to new dictionary\n",
    "with open('cso_dict_short.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for entity in entitySet:\n",
    "        writer.writerow({entity})\n",
    "\n",
    "print (\"Dictionary created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json file with 919 entities is created successfully!\n",
      "Total time: 0.32504400000016176\n"
     ]
    }
   ],
   "source": [
    "# query local ontology ('nt' format) with rdflib\n",
    "# export the hierarchy structure of current ontology\n",
    "# the max depth can be decided by users with MAX_DEPTH\n",
    "from rdflib import Graph\n",
    "import json\n",
    "import time\n",
    "\n",
    "MAX_DEPTH = 2\n",
    "totalEntity = 1\n",
    "\n",
    "def QueryChildren (parentURI, depth): \n",
    "    # constraint the max depth for testing\n",
    "    if depth > MAX_DEPTH:\n",
    "        return []\n",
    "    \n",
    "    results = g.query(\"\"\"\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n",
    "        SELECT ?children\n",
    "        WHERE { <\"\"\" + parentURI + \"\"\"> <http://cso.kmi.open.ac.uk/schema/cso#superTopicOf> ?children. }\n",
    "    \"\"\").serialize(format=\"json\")\n",
    "    results = json.loads(results)\n",
    "\n",
    "    # the termination case\n",
    "    if not len(results):\n",
    "        return []\n",
    "    \n",
    "    chilrenList = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        entity = {\n",
    "            \"name\": result[\"children\"][\"value\"],\n",
    "            \"uncertainty\" : 3\n",
    "        }\n",
    "        subChildren = QueryChildren (result[\"children\"][\"value\"], depth+1)\n",
    "        if len(subChildren):\n",
    "            entity.update( {\"children\" : subChildren} )\n",
    "        else:\n",
    "            entity.update( {\"size\" : 3} )\n",
    "        #print(result[\"children\"][\"value\"])\n",
    "        chilrenList.append(entity)\n",
    "        \n",
    "        global totalEntity\n",
    "        totalEntity = totalEntity+1\n",
    "\n",
    "    return chilrenList\n",
    "    \n",
    "g = Graph()\n",
    "g.parse(\"CSO.3.1.nt\", format=\"nt\")\n",
    "#g.parse(\"dbpedia.nt\", format=\"nt\")\n",
    "\n",
    "start = time.clock()\n",
    "csoHierarchy = {\n",
    "    \"name\": \"https://cso.kmi.open.ac.uk/topics/computer_science\",\n",
    "    \"uncertainty\" : 3,\n",
    "    \"children\": QueryChildren(\"https://cso.kmi.open.ac.uk/topics/computer_science\", 1)\n",
    "}\n",
    "\n",
    "#treeJson = FormatToJson(csoHierarchy)\n",
    "#print(treeJson)\n",
    "\n",
    "with open('../IdeaTest/bubble-treemaps/html/cso_hierarchy.json', 'w') as outfile:  \n",
    "    json.dump(csoHierarchy, outfile, indent = 2)\n",
    "\n",
    "print (\"Json file with \" + str(totalEntity) + \" entities is created successfully!\")\n",
    "print (\"Total time: \" + str(time.clock()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://cso.kmi.open.ac.uk/schema/cso#Topic\n"
     ]
    }
   ],
   "source": [
    "# query online ontology ('owl' format) with sparqlWrapper\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "#sparql = SPARQLWrapper(\"dbpedia.owl\")\n",
    "sparql = SPARQLWrapper(\"http://localhost:8890/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?\n",
    "    WHERE { <https://cso.kmi.open.ac.uk/topics/robotics> <<http://www.w3.org/2002/07/owl#sameAs>> <http://dbpedia.org/resource/Database> }\n",
    "\"\"\")\n",
    "# SELECT * WHERE {\n",
    "#        ?s ?p ?o .\n",
    "#}\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"label\"][\"value\"])\n",
    "    #print(result[\"thumb\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-7428b4e5cd3b>, line 276)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-7428b4e5cd3b>\"\u001b[0;36m, line \u001b[0;32m276\u001b[0m\n\u001b[0;31m    sampleSentence = \"During the past decade, researchers have explored the use of animation in many aspects of user interfaces. In 1984, the Apple Macintosh used rudimentary animation when opening and closing icons. This kind of animation was used to provide a continuous transition from one state of the interface to another, and has become increasingly common, both in research and commercial user interfaces. Users commonly report that they prefer animation, and yet there has been very little research that attempts to understand how animation affects users’ performance. A commonly held belief is that animation helps users maintain object constancy and thus helps users to relate the two states of the system. This notion was described well by Robertson and his colleagues in their paper on \"cone trees\", a 3D visualization technique that they developed. \"Interactive animation is used to shift some of the user’s cognitive load to the human perceptual system. ... The perceptual phenomenon of object constancy enables the user to track substructure relationships without thinking about it. When the animation is completed, no time is needed for reassimilation.\" [19 p. 191]. Researchers including Robertson have demonstrated through informal usability studies that animation can improve subjective user satisfaction. However, there have been few controlled studies looking specifically at how animation affects user performance. These studies are summarized below.\"\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import json\n",
    "\n",
    "import urllib\n",
    "#from owlready2 import *\n",
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from allennlp.common.testing import AllenNlpTestCase\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "# pre-processing\n",
    "def PreProcess(senSet):\n",
    "    #remove content between [ ]\n",
    "    print(\"Pre-processing...\")\n",
    "    for index in range(len(senSet)):\n",
    "        while senSet[index].find('[')>=0:\n",
    "            i_start = senSet[index].find('[')\n",
    "            i_end = senSet[index].find(']')\n",
    "            s = senSet[index][i_start:i_end+2]\n",
    "            senSet[index] = senSet[index].replace(s, \"\")\n",
    "\n",
    "\n",
    "def QueryURI(keywords, index=-2):\n",
    "    localSite = 'http://localhost:1111/api/search/KeywordSearch?'\n",
    "    onlineSite = 'http://lookup.dbpedia.org/api/search/KeywordSearch?'\n",
    "    prefix = \"{http://lookup.dbpedia.org/}\"\n",
    "    \n",
    "    keywords = keywords.replace(' ', \"%20\")\n",
    "    request = onlineSite + \\\n",
    "    'QueryClass='   + ''  + \\\n",
    "    '&MaxHits='     + '5' + \\\n",
    "    '&QueryString=' + keywords\n",
    "    response = str(urllib.request.urlopen(request).read(), 'utf-8')\n",
    "\n",
    "    root = ET.fromstring(response)\n",
    "    result = root.findall(prefix + \"Result\")\n",
    "    uriList = []\n",
    "    \n",
    "    if len(result)>0:\n",
    "        for entity in result:\n",
    "            uriList.append(entity.find(prefix + \"URI\").text);\n",
    "        return uriList\n",
    "    else:\n",
    "        print(\"Sorry, we find nothing for this stuff :(\\n\")\n",
    "        return None\n",
    "    \n",
    "    '''if len(result)>0:\n",
    "        selected = -1\n",
    "        count = 0\n",
    "        for name in result:\n",
    "            print(str(count) + \": \" + name.find(prefix + \"Label\").text)\n",
    "            count += 1\n",
    "        # for some default input during debugging\n",
    "        if index<-1:\n",
    "            index = int(input(\"Which one is closer to what you mean? (type \\\"-1\\\" if nothing seems correct) \"))\n",
    "        if index >= 0:\n",
    "            selected = \"<\" + result[index].find(prefix + \"URI\").text + \">\"\n",
    "        else:\n",
    "            selected = None\n",
    "        return selected.replace(\"/resource\", \"/ontology\")\n",
    "    else:\n",
    "        print(\"Sorry, we find nothing for this stuff :(\\n\")\n",
    "        return None'''\n",
    "\n",
    "            \n",
    "# get ontology hierarchy for every keyword and append the knowledge tree\n",
    "def AppendTree(URIList, treeDict):\n",
    "    for URI in URIList:\n",
    "        hierarchy = QueryHierarchy(URI);\n",
    "        #print(hierarchy)\n",
    "        \n",
    "        curDict = treeDict;\n",
    "        for curKey in hierarchy:\n",
    "            if curKey in curDict:\n",
    "                curDict = curDict[curKey]\n",
    "            else:\n",
    "                curDict[curKey] = dict()\n",
    "                curDict = curDict[curKey]\n",
    "    \n",
    "# A recursive helper function to traverse treeDict and format it to json\n",
    "def PreorderFormat(curDict):\n",
    "    if len(curDict) == 0:\n",
    "        return;\n",
    "    \n",
    "    childList = []\n",
    "    for key in curDict:\n",
    "        children = PreorderFormat(curDict[key])\n",
    "        if children:\n",
    "            childList.append({\n",
    "                \"name\": key,\n",
    "                \"uncertainty\": 3,\n",
    "                \"children\": children\n",
    "            })\n",
    "        else:\n",
    "            childList.append({\n",
    "                \"name\": key,\n",
    "                \"uncertainty\": 3,\n",
    "                \"size\": 10\n",
    "            })\n",
    "    return childList\n",
    "    \n",
    "        \n",
    "def FormatToJson(treeDict):\n",
    "    resultList = PreorderFormat(treeDict)\n",
    "    finalResult = None\n",
    "    \n",
    "    # only show the computer science part (remove math ...)\n",
    "    for result in resultList:\n",
    "        if \"computer_science\" in result[\"name\"]:\n",
    "            finalResult = result\n",
    "    \n",
    "    # show all part and add a root node\n",
    "    '''finalResult = {\n",
    "        \"name\": \"GroundRoot\",\n",
    "        \"uncertainty\": 3,\n",
    "        \"children\": resultList\n",
    "    }'''\n",
    "    \n",
    "    return finalResult\n",
    "    \n",
    "# extract one triple from given sentence\n",
    "def RunNER(sen):\n",
    "    # initialize the named entity list\n",
    "    entityList = []\n",
    "    \n",
    "    # parse sentence\n",
    "    doc = nlp(str(sen))\n",
    "    print('\\n' + str(index) + '. Original Sentence:\\n' + sen)\n",
    "\n",
    "    #ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "    chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if \"subj\" in chunk.root.dep_ or \"obj\" in chunk.root.dep_:\n",
    "            # test whether current chunk is or contains stop words\n",
    "            result = ''\n",
    "            doc_phrase = nlp(chunk.text)\n",
    "            for token in doc_phrase:\n",
    "                #print(token.text, token.is_stop, token.lemma_)\n",
    "                if not token.is_stop and token.lemma_ != \"-PRON-\":\n",
    "                # exclude stop words and personal pronouns (whose lemma_ is \"-PRON-\")\n",
    "                    result = result + token.text + ' '\n",
    "            \n",
    "            if result != '':\n",
    "                chunks.append(result[:-1])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# given a URI in DBPedia, query corresponding URI in CSO\n",
    "def DBPD2CSO(dbpediaURI):\n",
    "    csoURIList = []\n",
    "    \n",
    "    results = csoGraph.query(\"\"\"\n",
    "        SELECT ?csoURI\n",
    "        WHERE { ?csoURI <http://www.w3.org/2002/07/owl#sameAs> <\"\"\" + dbpediaURI + \"\"\">. }\n",
    "    \"\"\").serialize(format=\"json\")\n",
    "    results = json.loads(results)\n",
    "\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        csoURIList.append('<' + result[\"csoURI\"][\"value\"] + '>');\n",
    "        \n",
    "    return csoURIList\n",
    "\n",
    "# given a list of candidate uri, select the best one\n",
    "def SelectURI(source, candiList):\n",
    "    maxSim = 0;\n",
    "    maxURI = candiList[0]\n",
    "    \n",
    "    for candidate in candiList:\n",
    "        print(source[source.rfind(\"/\")+1: -1])\n",
    "        print(candidate[candidate.rfind(\"/\")+1: -1])\n",
    "        w1 = wordnet.synsets(source[source.rfind(\"/\")+1: -1])\n",
    "        w2 = wordnet.synsets(candidate[candidate.rfind(\"/\")+1: -1])\n",
    "        if len(w1) and len(w2):\n",
    "            similarity = w1[0].wup_similarity(w2[0])\n",
    "            print(similarity)\n",
    "            if similarity > maxSim:\n",
    "                maxSim = similarity\n",
    "                maxURI = candidate\n",
    "                print(candidate)\n",
    "                print(similarity)\n",
    "    return maxURI\n",
    "\n",
    "# given a URI, query the ontology iteratively to get its path to root\n",
    "def QueryHierarchy(URI):\n",
    "    print(URI)\n",
    "    path = []\n",
    "    path.insert(0, URI)\n",
    "    \n",
    "    curURI = URI\n",
    "    endFlag = False # to mark whether a dbo:entity is found in current level\n",
    "    \n",
    "    while not endFlag:\n",
    "        endFlag = True\n",
    "        \n",
    "        qSelect = \"\"\"\n",
    "            SELECT ?parentURI\n",
    "            WHERE { ?parentURI <http://cso.kmi.open.ac.uk/schema/cso#superTopicOf> \"\"\" + curURI + \"\"\". }\n",
    "        \"\"\"\n",
    "\n",
    "        results = csoGraph.query(qSelect).serialize(format=\"json\")\n",
    "        results = json.loads(results)\n",
    "\n",
    "        \n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            resultURI = '<' + result[\"parentURI\"][\"value\"] + '>'\n",
    "            #print(resultURI)\n",
    "            curURI = resultURI\n",
    "            path.insert(0, resultURI)\n",
    "            endFlag = False\n",
    "            break;\n",
    "     \n",
    "    # insert the common root node to current path\n",
    "    # path.insert(0, '<https://cso.kmi.open.ac.uk/topics/computer_science>')\n",
    "    print(path)\n",
    "    return path\n",
    "        \n",
    "# load Spacy NLP dictionary\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# load DBPD ontology and construct graph for query\n",
    "#m_world = World()# Owlready2 stores every triples in a ‘World’ object\n",
    "#m_onto = m_world.get_ontology(\"dbpedia.owl\").load()\n",
    "#m_graph = m_world.as_rdflib_graph()\n",
    "sparql = SPARQLWrapper(\"http://localhost:8890/sparql\")\n",
    "#sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "csoGraph = Graph()\n",
    "csoGraph.parse(\"CSO.3.1.nt\", format=\"nt\")\n",
    "\n",
    "# load data\n",
    "file = open(\"shortdataset.csv\", \"r\")\n",
    "#file = open(\"newdataset_formatted.csv\", \"r\")\n",
    "reader = csv.reader(file)\n",
    "senSet = []\n",
    "for item in reader:\n",
    "    # format sentences in item as string\n",
    "    fullP = \"\".join(item)\n",
    "    splitP = fullP.split(\";\", 3);\n",
    "    splitS = splitP[3][1:len(splitP[3])].split(\".\");\n",
    "    #print(splitS)\n",
    "    for sen in splitS:\n",
    "        senSet.append(sen)#store the sentence into an array\n",
    "file.close()\n",
    "print(\"Total sentences: \" + str(len(senSet)))\n",
    "\n",
    "# pre-processing\n",
    "PreProcess(senSet)\n",
    "\n",
    "cacheDict = dict()\n",
    "# parse and query each sentence\n",
    "entityList = []\n",
    "#for index in range(10, 20):\n",
    "#for index in range(len(senSet)):\n",
    "index = 26\n",
    "sampleSentence = \"We examine how animating a viewpoint change in a spatial \\\n",
    "information system affects a user’s ability to build a mental \\\n",
    "map of the information in the space. We found that \\\n",
    "animation improves users' ability to reconstruct the \\\n",
    "information space, with no penalty on task performance \\\n",
    "time. We believe that this study provides strong evidence \\\n",
    "for adding animated transitions in many applications with \\\n",
    "fixed spatial data where the user navigates around the data \\\n",
    "space.\"\n",
    "'''sampleSentence = \"We believe that this study provides strong evidence \\\n",
    "for adding animated transitions in many applications with \\\n",
    "fixed spatial data where the user navigates around the data \\\n",
    "space.\"'''\n",
    "\n",
    "# extract named entities from current sentence\n",
    "entityList = RunNER(sampleSentence)\n",
    "#entityList = RunNER(senSet[index])\n",
    "print(entityList)\n",
    "\n",
    "# look up the URI for the entities\n",
    "URIList = []\n",
    "for entity in entityList:\n",
    "    print(\"\\nFor \\\"\" + entity + \"\\\":\")\n",
    "    try:\n",
    "        if entity in cacheDict:\n",
    "            entityURI = cacheDict[entity];\n",
    "            if entityURI != None: \n",
    "                print(\"You mentioned\", entity, \"before. Do you mean\", entityURI, \"?\")\n",
    "            else:\n",
    "                print(\"You mentioned\", entity, \"before, but we can't find anything about it.\")\n",
    "\n",
    "        else:\n",
    "            entityURI = QueryURI(entity)\n",
    "            #print(entityURI)\n",
    "            #cacheDict[entity] = entityURI\n",
    "\n",
    "        print(\"\\n\")\n",
    "        #print(\"URI: \" + entityURI[1:len(entityURI)-1])\n",
    "        if entityURI != None:\n",
    "            #URIList.append(entityURI)\n",
    "            csoURIList = []\n",
    "            for dbpediaURI in entityURI:\n",
    "                csoURIList.extend(DBPD2CSO(dbpediaURI))\n",
    "            if len(csoURIList):\n",
    "                URIList.append(SelectURI(entity, csoURIList))\n",
    "    except:\n",
    "        print(\"none\")\n",
    "\n",
    "print(URIList)\n",
    "\n",
    "# output the concatenated hierarchy\n",
    "treeDict = dict()\n",
    "if len(URIList)>0:\n",
    "    AppendTree(URIList, treeDict)\n",
    "\n",
    "treeJson = FormatToJson(treeDict)\n",
    "print(treeJson)\n",
    "\n",
    "with open('../IdeaTest/bubble-treemaps/html/cso_query_result.json', 'w') as outfile:  \n",
    "    json.dump(treeJson, outfile, indent = 2)\n",
    "\n",
    "'''\n",
    "# output separated information for each entity\n",
    "outputList = []\n",
    "if len(URIList)>0:\n",
    "    for URI in URIList:\n",
    "        entityInfo = {\n",
    "            \"uri\": URI,\n",
    "            \"strPath\": \"\",\n",
    "            \"sentence\": senSet[index],\n",
    "            \"abstract\": None,\n",
    "            \"thumbnail\": None\n",
    "        }\n",
    "        hierarchy = QueryHierarchy(URI)\n",
    "        for curKey in hierarchy:\n",
    "            entityInfo[\"strPath\"] = entityInfo[\"strPath\"]  + curKey + \"&-&\"\n",
    "        entityInfo[\"strPath\"] = entityInfo[\"strPath\"][:-3]\n",
    "        #QueryInfo(URI, entityInfo)\n",
    "        outputList.append(entityInfo)\n",
    "\n",
    "print(outputList)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "    \n",
    "w1 = wordnet.synsets(\"%$3\")\n",
    "w2 = wordnet.synsets(\"cat\")\n",
    "print(w1)\n",
    "print(w2)\n",
    "#print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequent_calculus\n"
     ]
    }
   ],
   "source": [
    "source = \"<https://cso.kmi.open.ac.uk/topics/sequent_calculus>\"\n",
    "source = source[source.rfind(\"/\")+1: -1]\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Root Level', 'children': [{'name': 'Top Level', 'children': [{'name': 'TestTTTTT', 'children': [{'name': 'Child1 of A'}, {'name': 'Child2 of A', 'children': [{'name': 'Child1 of Child2'}, {'name': 'Child2 of Child2'}, {'name': 'Child3 of Child2'}]}, {'name': 'Child3 of A'}, {'name': 'Child4 of A'}, {'name': 'Child5 of A'}]}, {'name': 'Level 2: B', 'children': [{'name': 'Child1 of B'}, {'name': 'Child2 of B'}, {'name': 'Child3 of B'}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "d = {\n",
    "    \"name\": \"Root Level\",\n",
    "    \"children\": [{ \n",
    "        \"name\": \"Top Level\",\n",
    "        \"children\": [{ \n",
    "            \"name\": \"Level 2: A\",\n",
    "                \"children\": [\n",
    "                    { \"name\": \"Child1 of A\" },\n",
    "                    { \"name\": \"Child2 of A\",\n",
    "                        \"children\": [\n",
    "                            { \"name\": \"Child1 of Child2\" },\n",
    "                            { \"name\": \"Child2 of Child2\" },\n",
    "                            { \"name\": \"Child3 of Child2\" }\n",
    "                        ]\n",
    "                    },\n",
    "                    { \"name\": \"Child3 of A\" },\n",
    "                    { \"name\": \"Child4 of A\" },\n",
    "                    { \"name\": \"Child5 of A\" }\n",
    "                ]\n",
    "            },\n",
    "            { \n",
    "                \"name\": \"Level 2: B\",\n",
    "                \"children\": [\n",
    "                    { \"name\": \"Child1 of B\" },\n",
    "                    { \"name\": \"Child2 of B\" },\n",
    "                    { \"name\": \"Child3 of B\" }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "}\n",
    "\n",
    "# print(d)\n",
    "\n",
    "node = d[\"children\"][0][\"children\"][0]\n",
    "node[\"name\"] = \"TestTTTTT\"\n",
    "\n",
    "print(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
