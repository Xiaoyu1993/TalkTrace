{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 46\n",
      "Pre-processing...\n",
      "0: Sure um start off - I'm a Purdue grad\n",
      "stop words: off, a, \n",
      "\n",
      "I  - be - Purdue grad \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from __future__ import print_function\n",
    "\n",
    "# pre-processing\n",
    "def filter(senSet):\n",
    "    #remove content between [ ]\n",
    "    print(\"Pre-processing...\")\n",
    "    for index in range(len(senSet)):\n",
    "        while senSet[index].find('[')>=0:\n",
    "            i_start = senSet[index].find('[')\n",
    "            i_end = senSet[index].find(']')\n",
    "            s = senSet[index][i_start:i_end+2]\n",
    "            senSet[index] = senSet[index].replace(s, \"\")\n",
    "\n",
    "#load data\n",
    "file = open(\"shortdataset.csv\", \"r\")\n",
    "#file = open(\"newdataset_formatted.csv\", \"r\")\n",
    "reader = csv.reader(file)\n",
    "\n",
    "senSet = []\n",
    "for item in reader:\n",
    "    #format sentences in item as string\n",
    "    fullP = \"\".join(item)\n",
    "    splitP = fullP.split(\";\", 3);\n",
    "    splitS = splitP[3][1:len(splitP[3])].split(\".\");\n",
    "    #print(splitS)\n",
    "    for sen in splitS:\n",
    "        senSet.append(sen)#store the sentence into an array\n",
    "\n",
    "file.close()\n",
    "print(\"Total sentences: \" + str(len(senSet)))\n",
    "\n",
    "#pre-processing\n",
    "filter(senSet)\n",
    "\n",
    "'''for sentence in senSet:\n",
    "    print(senSet)'''\n",
    "\n",
    "#parse sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#for index in range(len(senSet)):\n",
    "index = 0\n",
    "doc = nlp(str(senSet[index]))\n",
    "print(str(index) + ': ' + senSet[index])\n",
    "\n",
    "#print the result\n",
    "sub = \"\"\n",
    "pred = \"\"\n",
    "obj = \"\"\n",
    "stop_index = []\n",
    "\n",
    "print('stop words: ', end='')\n",
    "for token in doc:\n",
    "    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #      token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "    # record the index of stop words\n",
    "    if token.is_stop:\n",
    "        print(token.text + ', ', end='')\n",
    "        stop_index.append(token.i)\n",
    "    if re.match('nsubj', token.dep_):   \n",
    "        subj = token.text\n",
    "    if re.match('ROOT', token.dep_): \n",
    "        pred = token.lemma_\n",
    "        pred_orig = token.text\n",
    "    if re.match('dobj', token.dep_): \n",
    "        obj = token.lemma_\n",
    "print('\\n')\n",
    "\n",
    "# using chunk to update subject and object\n",
    "for chunk in doc.noun_chunks:\n",
    "    if chunk.root.head.text == pred_orig and re.match('nsubj', chunk.root.dep_):\n",
    "        subj = chunk.text\n",
    "        # remove stop words\n",
    "        '''i_stop=0\n",
    "        for i_sen in range(chunk.start, chunk.end):\n",
    "            while i_stop < len(stop_index) and stop_index[i_stop] < i_sen-1:\n",
    "                #print(str(stop_index[i_stop]) + ' ' + str(i_sen))\n",
    "                i_stop = i_stop+1\n",
    "            # there is no stop word in current chunk\n",
    "            if i_stop >= len(stop_index):\n",
    "                break;\n",
    "            #print(i_sen)\n",
    "            # finish going through the chunk\n",
    "            if stop_index[i_stop] > chunk.end-1:\n",
    "                break\n",
    "            # find the stop word and remove it\n",
    "            if stop_index[i_stop] == i_sen-1:\n",
    "                #print(doc[i_sen-1])\n",
    "                if i_sen-1 == chunk.start:\n",
    "                    subj = subj.replace(doc[i_sen-1].text + ' ', '')\n",
    "                else:\n",
    "                    subj = subj.replace(' ' + doc[i_sen-1].text, '')'''\n",
    "\n",
    "    if chunk.root.head.text == pred_orig and re.match('dobj|attr', chunk.root.dep_):\n",
    "        obj = chunk.text\n",
    "        # remove stop words\n",
    "        '''i_stop=0\n",
    "        for i_sen in range(chunk.start, chunk.end):\n",
    "            while i_stop < len(stop_index) and stop_index[i_stop] < i_sen-1:\n",
    "                #print(str(stop_index[i_stop]) + ' ' + str(i_sen))\n",
    "                i_stop = i_stop+1\n",
    "            # there is no stop word in current chunk\n",
    "            if i_stop >= len(stop_index):\n",
    "                break;\n",
    "            #print(i_sen)\n",
    "            # finish going through the chunk\n",
    "            if stop_index[i_stop] > chunk.end-1:\n",
    "                break\n",
    "            # find the stop word and remove it\n",
    "            if stop_index[i_stop] == i_sen-1:\n",
    "                #print(doc[i_sen-1])\n",
    "                if i_sen-1 == chunk.start:\n",
    "                    obj = obj.replace(doc[i_sen-1].text + ' ', '')\n",
    "                else:\n",
    "                    obj = obj.replace(' ' + doc[i_sen-1].text, '')'''\n",
    "\n",
    "    #print(chunk.text + ' ' + str(chunk.start))\n",
    "    #print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "\n",
    "#print(subj + ' - ' + pred + ' - ' + obj + '\\n')\n",
    "\n",
    "# second method to remove stop words\n",
    "subj_new = ''\n",
    "doc_subj = nlp(subj)\n",
    "for token in doc_subj:\n",
    "    #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #      token.shape_, token.is_alpha, token.is_stop)\n",
    "    if ~token.is_stop:\n",
    "        subj_new = subj_new + token.text + ' '\n",
    "\n",
    "obj_new = ''\n",
    "doc_obj = nlp(obj)\n",
    "for token in doc_obj:\n",
    "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #       token.shape_, token.is_alpha, token.is_stop)\n",
    "    if not token.is_stop:\n",
    "        obj_new = obj_new + token.text + ' '\n",
    "print(subj_new + ' - ' + pred + ' - ' + obj_new + '\\n')\n",
    "\n",
    "## visualize the semantic tree\n",
    "#options = {'compact': True, 'color': 'blue'}\n",
    "#displacy.serve(doc, style='dep', options=options)\n",
    "#displacy.serve(doc, style='dep')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
