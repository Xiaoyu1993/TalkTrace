{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/senator belongs to more than one entity types (e.g. Class, Property, Individual): [owl.ObjectProperty, dbpedia.MemberOfParliament, DUL.sameSettingAs]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/politicGovernmentDepartment belongs to more than one entity types (e.g. Class, Property, Individual): [owl.ObjectProperty, dbpedia.Department, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/productShape belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, DUL.hasQuality]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/latinName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.Name]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6391Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6393Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6392Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/ingredientName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/greekName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.Name]; I'm trying to fix it...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 46\n",
      "Pre-processing...\n",
      "\n",
      "2:   Um we manage all the construction and innovation projects for the university across the state\n",
      "stop words: we, all, the, and, for, the, across, the, \n",
      "\n",
      "Before : we - manage - all the construction and innovation projects\n",
      "Method1: we - manage - construction innovation projects\n",
      "Method2: - manage - construction innovation projects \n",
      "\n",
      "Triple for Query   : we - manage - all the construction and innovation projects\n",
      "Triple Query Result: [False]\n",
      "all all ADJ PDT predet xxx True True\n",
      "the the DET DT det xxx True True\n",
      "construction construction NOUN NN nmod xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "innovation innovation NOUN NN conj xxxx True False\n",
      "projects project NOUN NNS ROOT xxxx True False\n",
      "all the construction and innovation projects projects ROOT projects\n",
      "here1\n",
      "here2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'here2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from owlready2 import *\n",
    "import rdflib\n",
    "\n",
    "import re\n",
    "\n",
    "# pre-processing\n",
    "def PreProcess(senSet):\n",
    "    #remove content between [ ]\n",
    "    print(\"Pre-processing...\")\n",
    "    for index in range(len(senSet)):\n",
    "        while senSet[index].find('[')>=0:\n",
    "            i_start = senSet[index].find('[')\n",
    "            i_end = senSet[index].find(']')\n",
    "            s = senSet[index][i_start:i_end+2]\n",
    "            senSet[index] = senSet[index].replace(s, \"\")\n",
    "            \n",
    "# stopwords from parsing the whole sentence\n",
    "def RemoveStopword1(phrase, doc, chunkStart, chunkEnd, stopList):\n",
    "    result = phrase\n",
    "    i_stop=0\n",
    "    #start = chunk.start# to eliminate the condition when the first word of chunk is stop word\n",
    "    for i_sen in range(chunkStart, chunkEnd):\n",
    "        while i_stop < len(stopList) and stopList[i_stop] < i_sen-1:\n",
    "            #print(str(stopList[i_stop]) + ' ' + str(i_sen))\n",
    "            i_stop = i_stop+1\n",
    "        # there is no stop word in current chunk\n",
    "        if i_stop >= len(stopList):\n",
    "            break;\n",
    "        #print(i_sen)\n",
    "        # finish going through the chunk\n",
    "        if stopList[i_stop] > chunkEnd-1:\n",
    "            break\n",
    "        # find the stop word and remove it\n",
    "        if stopList[i_stop] == i_sen-1:\n",
    "            #print(doc[i_sen-1])\n",
    "            if i_sen-1 == chunkStart:\n",
    "                result = result.replace(doc[i_sen-1].text + ' ', '')\n",
    "                chunkStart = chunkStart+1\n",
    "            else:\n",
    "                result = result.replace(' ' + doc[i_sen-1].text, '')\n",
    "    return result\n",
    "\n",
    "# stopwords from parsing triple separately\n",
    "def RemoveStopword2(inputPhrase):\n",
    "    result = ''\n",
    "    doc_phrase = nlp(str(inputPhrase))\n",
    "    for token in doc_phrase:\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #       token.shape_, token.is_alpha, token.is_stop)\n",
    "        if not token.is_stop:\n",
    "            result = result + token.text + ' '\n",
    "        #else:\n",
    "        #    print(token.text + ', ', end = '')    \n",
    "    return result\n",
    "\n",
    "\n",
    "# extract one triple from given sentence\n",
    "def ExtractTriple(sen):\n",
    "    # initialize the triple and stop word list\n",
    "    subj = \"\"\n",
    "    pred = \"\"\n",
    "    obj = \"\"\n",
    "    stopList = []\n",
    "    \n",
    "    # parse sentence\n",
    "    doc = nlp(str(sen))\n",
    "    print('\\n' + str(index) + ': ' + senSet[index])\n",
    "    \n",
    "    ## visualize the semantic tree\n",
    "    #options = {'compact': True, 'color': 'blue'}\n",
    "    #displacy.serve(doc, style='dep', options=options)\n",
    "    #displacy.serve(doc, style='dep')\n",
    "\n",
    "    print('stop words: ', end='')\n",
    "    for token in doc:\n",
    "        #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #      token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "        # record the index of stop words\n",
    "        if token.is_stop:\n",
    "            print(token.text + ', ', end='')\n",
    "            stopList.append(token.i)\n",
    "        if re.match('nsubj', token.dep_):   \n",
    "            subj = token.text\n",
    "        if re.match('ROOT', token.dep_): \n",
    "            pred = token.lemma_\n",
    "            pred_orig = token.text\n",
    "        if re.match('dobj', token.dep_): \n",
    "            obj = token.text\n",
    "            '''#an earlier solution that I find not necessary\n",
    "            obj = token.lemma_\n",
    "            # to avoid cases like \"-PRON-\"\n",
    "            if obj[0] == '-':\n",
    "                obj = token.text'''\n",
    "    print('\\n')\n",
    "\n",
    "    subj_1 = subj\n",
    "    obj_1 = obj\n",
    "    # using chunk to update subject and object\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.head.text == pred_orig and re.match('nsubj', chunk.root.dep_):\n",
    "            subj = chunk.text\n",
    "            # remove stop words\n",
    "            subj_1 = RemoveStopword1(subj, doc, chunk.start, chunk.end, stopList)\n",
    "\n",
    "        if chunk.root.head.text == pred_orig and re.match('dobj|attr', chunk.root.dep_):\n",
    "            obj = chunk.text\n",
    "            # remove stop words\n",
    "            obj_1 = RemoveStopword1(obj, doc, chunk.start, chunk.end, stopList)\n",
    "        #print(chunk.text + ' ' + str(chunk.start))\n",
    "        #print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "\n",
    "    print('Before : ' + subj + ' - ' + pred + ' - ' + obj)\n",
    "    print('Method1: ' + subj_1 + ' - ' + pred + ' - ' + obj_1)\n",
    "\n",
    "    # second method to remove stop words\n",
    "    subj_2 = RemoveStopword2(subj)\n",
    "    obj_2 = RemoveStopword2(obj)\n",
    "    print('Method2: ' + subj_2 + '- ' + pred + ' - ' + obj_2 + '\\n')\n",
    "\n",
    "    return [subj, pred, obj]\n",
    "\n",
    "# transfer a phrase to a URI form\n",
    "def FormatURI(phrase):\n",
    "    #print('Before formatting:  ' + phrase)\n",
    "    chars = list(phrase)\n",
    "    if len(chars) > 0:\n",
    "        chars[0] = chars[0].upper()\n",
    "    for i in range(len(chars)):\n",
    "        if chars[i] == ' ' and i+1 < len(chars):\n",
    "            chars[i+1] = chars[i+1].upper()\n",
    "    phrase = ''.join(chars)\n",
    "    phrase = phrase.replace(' ', '')\n",
    "    phrase = re.sub(r'[^a-zA-Z0-9\\s]', '', phrase)\n",
    "    #print('After formatting:  ' + phrase)\n",
    "    return phrase\n",
    "\n",
    "# query the given triple in the ontology with SPARQL\n",
    "# return true/false as result\n",
    "def QueryTriple(subj, pred, obj):\n",
    "    prefix = \"\"\"\n",
    "    PREFIX rdf:<http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX dbpd:<http://dbpedia.org/ontology/>\n",
    "    \"\"\"\n",
    "    #subj = \"provinceLink\"\n",
    "    #pred = \"range\"\n",
    "    #obj = \"Province\"\n",
    "    qSelect = prefix + \"\"\"\n",
    "    SELECT ?sub WHERE {\n",
    "      ?sub rdf:\"\"\" + FormatURI(pred) + \"\"\" dbpd:\"\"\" + FormatURI(obj) + \"\"\".\n",
    "    }\"\"\"\n",
    "    qAsk = prefix + \"\"\"\n",
    "    ASK {\n",
    "        dbpd:\"\"\" + FormatURI(subj) + \"\"\" rdf:\"\"\" + FormatURI(pred) + \"\"\" dbpd:\"\"\" + FormatURI(obj) + \"\"\".\n",
    "    }\"\"\"\n",
    "    \n",
    "    r = list(m_graph.query(qAsk))\n",
    "    return r\n",
    "\n",
    "def PatialQuery(subj, pred, obj):\n",
    "    doc = nlp(str(obj))\n",
    "    for token in doc:\n",
    "        #rint(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "        #     token.shape_, token.is_alpha, token.is_stop)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)\n",
    "\n",
    "    r = 'here1'\n",
    "    print(r)\n",
    "    return r\n",
    "\n",
    "def ComponentQuery(subj, pred, obj):\n",
    "    r = 'here2'\n",
    "    print(r)\n",
    "    return r\n",
    "\n",
    "# load Spacy NLP dictionary\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# load DBPD ontology and construct graph for query\n",
    "m_world = World()# Owlready2 stores every triples in a ‘World’ object\n",
    "m_onto = m_world.get_ontology(\"dbpedia.owl\").load()\n",
    "m_graph = m_world.as_rdflib_graph()\n",
    "\n",
    "# load data\n",
    "file = open(\"shortdataset.csv\", \"r\")\n",
    "#file = open(\"newdataset_formatted.csv\", \"r\")\n",
    "reader = csv.reader(file)\n",
    "senSet = []\n",
    "for item in reader:\n",
    "    #format sentences in item as string\n",
    "    fullP = \"\".join(item)\n",
    "    splitP = fullP.split(\";\", 3);\n",
    "    splitS = splitP[3][1:len(splitP[3])].split(\".\");\n",
    "    #print(splitS)\n",
    "    for sen in splitS:\n",
    "        senSet.append(sen)#store the sentence into an array\n",
    "file.close()\n",
    "print(\"Total sentences: \" + str(len(senSet)))\n",
    "\n",
    "# pre-processing\n",
    "PreProcess(senSet)\n",
    "\n",
    "# parse and query each sentence\n",
    "#or index in range(len(senSet)):\n",
    "index = 2\n",
    "\n",
    "# extract triple from current sentence\n",
    "[subj, pred, obj] = ExtractTriple(senSet[index])\n",
    "print('Triple for Query   : ' + subj + ' - ' + pred + ' - ' + obj)\n",
    "\n",
    "# query the triple in dbpd with SPARQL\n",
    "queryResult = QueryTriple(subj, pred, obj)\n",
    "print('Triple Query Result: ' + str(queryResult))\n",
    "\n",
    "# query with only a part of the triple\n",
    "PatialQuery(subj, pred, obj)\n",
    "\n",
    "ComponentQuery(subj, pred, obj)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove WARNINGs from Owlready2\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/senator belongs to more than one entity types (e.g. Class, Property, Individual): [owl.ObjectProperty, dbpedia.MemberOfParliament, DUL.sameSettingAs]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/politicGovernmentDepartment belongs to more than one entity types (e.g. Class, Property, Individual): [owl.ObjectProperty, dbpedia.Department, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/productShape belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, DUL.hasQuality]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/latinName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.Name]; I'm trying to fix it...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6391Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6393Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/iso6392Code belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.LanguageCode]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/ingredientName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, DUL.hasPart]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: http://dbpedia.org/ontology/greekName belongs to more than one entity types (e.g. Class, Property, Individual): [owl.DataProperty, dbpedia.Name]; I'm trying to fix it...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "DESCRIBE not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b27ce1a90687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m DESCRIBE ?sub WHERE {\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m?\u001b[0m\u001b[0msub\u001b[0m \u001b[0mrdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrange\u001b[0m  \u001b[0mdbpd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mProvince\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m }\"\"\"))\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/rdflib/graph.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query_object, processor, result, initNs, initBindings, use_store_provided, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         return result(processor.query(\n\u001b[0;32m-> 1089\u001b[0;31m             query_object, initBindings, initNs, **kwargs))\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m     def update(self, update_object, processor='sparql',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/rdflib/plugins/sparql/processor.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, strOrQuery, initBindings, initNs, base, DEBUG)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrOrQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mevalQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitBindings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/rdflib/plugins/sparql/evaluate.py\u001b[0m in \u001b[0;36mevalQuery\u001b[0;34m(graph, query, initBindings, base)\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mevalPart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/rdflib/plugins/sparql/evaluate.py\u001b[0m in \u001b[0;36mevalPart\u001b[0;34m(ctx, part)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DescribeQuery'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DESCRIBE not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: DESCRIBE not implemented"
     ]
    }
   ],
   "source": [
    "# reference: https://pythonhosted.org/Owlready2/world.html\n",
    "from owlready2 import *\n",
    "import rdflib\n",
    "\n",
    "my_world = World()# Owlready2 stores every triples in a ‘World’ object\n",
    "onto = my_world.get_ontology(\"dbpedia.owl\").load()\n",
    "\n",
    "graph = my_world.as_rdflib_graph()\n",
    "print(len(graph))\n",
    "\n",
    "prefix = \"\"\"\n",
    "PREFIX rdf:<http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX dbpd:<http://dbpedia.org/ontology/>\n",
    "\"\"\"\n",
    "\n",
    "'''r = list(graph.query(prefix + \"\"\"\n",
    "SELECT ?sub WHERE {\n",
    "  ?sub rdf:range  dbpd:Province.\n",
    "}\"\"\"))'''\n",
    "\n",
    "'''r = list(graph.query(prefix + \"\"\"\n",
    "ASK {\n",
    "  dbpd:provinceLink rdf:range  dbpd:Province.\n",
    "}\"\"\"))'''\n",
    "\n",
    "r = list(graph.query(prefix + \"\"\"\n",
    "DESCRIBE ?sub WHERE {\n",
    "  ?sub rdf:range  dbpd:Province.\n",
    "}\"\"\"))\n",
    "\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
